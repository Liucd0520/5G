{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/data/anaconda/envs/monai-1.0.1/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "import glob\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    AddChanneld,\n",
    "    SpatialPadd,\n",
    "    RandRotate90d,\n",
    "    RandShiftIntensityd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    MapTransform,\n",
    "    Resized,\n",
    "    Invertd,\n",
    "    ToTensord,\n",
    "    NormalizeIntensityd,\n",
    "    RandFlipd,\n",
    "    Lambdad,\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    ")\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.data import CacheDataset, ThreadDataLoader,DataLoader, Dataset, decollate_batch,load_decathlon_datalist\n",
    "import torch\n",
    "from monai.utils import first, set_determinism\n",
    "import torch.nn as  nn\n",
    "from torch.nn import Linear,  Softmax\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "set_determinism(seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_raw = pd.read_csv('判定_fill_df.csv')\n",
    "df_cli = df_raw[['patient_ID', 'T_stage', 'HER2_status', 'NAC_classification', 'ER_percentage', 'PR_percentage', 'Ki_67']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_adc': '/app/liucd/deeplearn_dec/DL_dec/data_adc/zunyi/Mixed/2022_10_20_1650366_ADC2_0000_0.nii.gz', 'image_dce': '/app/liucd/deeplearn_dec/DL_dec/data/zunyi/Mixed/2022_10_20_1650366_+C2_0000_0.nii.gz', 'clinical': [3.0, 1.0, 2.0, 0.0, 0.0, 0.2], 'label': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(656, 225, 881)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syf1_adcdir = '/app/liucd/deeplearn_dec/DL_dec/data_adc/syf_stage1/Mixed'\n",
    "syf2_adcdir = '/app/liucd/deeplearn_dec/DL_dec/data_adc/syf_stage2/Mixed'\n",
    "zy_adcdir = '/app/liucd/deeplearn_dec/DL_dec/data_adc/zunyi/Mixed'\n",
    "\n",
    "syf1_dcedir = '/app/liucd/deeplearn_dec/DL_dec/data/syf_stage1/Mixed'\n",
    "syf2_dcedir = '/app/liucd/deeplearn_dec/DL_dec/data/syf_stage2/Mixed'\n",
    "zy_dcedir = '/app/liucd/deeplearn_dec/DL_dec/data/zunyi/Mixed'\n",
    "\n",
    "train_adcimages = sorted(glob.glob(os.path.join(syf1_adcdir,  '*.nii.gz'))) + \\\n",
    "                 sorted(glob.glob(os.path.join(syf2_adcdir,  '*.nii.gz'))) \n",
    "\n",
    "train_dceimages = sorted(glob.glob(os.path.join(syf1_dcedir,  '*.nii.gz'))) + \\\n",
    "                 sorted(glob.glob(os.path.join(syf2_dcedir,  '*.nii.gz'))) \n",
    "\n",
    "val_adcimages = sorted(glob.glob(os.path.join(zy_adcdir,  '*.nii.gz'))) \n",
    "val_dceimages =  sorted(glob.glob(os.path.join(zy_dcedir,  '*.nii.gz')))\n",
    "\n",
    "\n",
    "train_clinical = []\n",
    "for file_path in train_adcimages:\n",
    "    p_id = file_path.split('_')[-4]\n",
    "    clinical_data = df_cli[df_cli['patient_ID'] == int(p_id)].values.tolist()[0][1:]\n",
    "    train_clinical.append(clinical_data)\n",
    "\n",
    "val_clinical = []\n",
    "for file_path in val_adcimages:\n",
    "    p_id = file_path.split('_')[-4]\n",
    "    clinical_data = df_cli[df_cli['patient_ID'] == int(p_id)].values.tolist()[0][1:]\n",
    "    val_clinical.append(clinical_data)\n",
    "    \n",
    "\n",
    "train_dict = [{'image_adc': image_adc, 'image_dce': image_dce, 'clinical': clinical,  'label': int(image_adc.split('_')[-1].replace('.nii.gz', ''))} \n",
    "                  for image_adc, image_dce, clinical in zip(train_adcimages,  train_dceimages, train_clinical)]\n",
    "val_dict = [{'image_adc': image_adc, 'image_dce': image_dce, 'clinical': clinical,  'label': int(image_adc.split('_')[-1].replace('.nii.gz', ''))} \n",
    "                  for image_adc, image_dce, clinical in zip(val_adcimages, val_dceimages, val_clinical)]\n",
    "\n",
    "print(train_dict[-1])\n",
    "len(train_dict), len(val_dict), len(train_dict + val_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████| 656/656 [00:53<00:00, 12.21it/s]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████| 225/225 [00:11<00:00, 19.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image_adc\", \"image_dce\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image_adc\", \"image_dce\"]),\n",
    "            Orientationd(keys=[\"image_adc\", 'image_dce'], axcodes=\"RAS\"),\n",
    "            Resized(keys=[\"image_adc\"], spatial_size=(64, 64, 16)),\n",
    "            Resized(keys=[\"image_dce\"], spatial_size=(96, 96, 32)),\n",
    "            \n",
    "            NormalizeIntensityd(keys=[\"image_adc\", \"image_dce\"], nonzero=True, channel_wise=True),\n",
    "            \n",
    "            RandFlipd( keys=[\"image_adc\", ], spatial_axis=[0], prob=0.50),\n",
    "            RandFlipd( keys=[\"image_adc\", ], spatial_axis=[1], prob=0.50),\n",
    "            RandFlipd( keys=[\"image_adc\", ], spatial_axis=[2], prob=0.50),\n",
    "            \n",
    "            RandFlipd( keys=[\"image_dce\", ], spatial_axis=[0], prob=0.50),\n",
    "            RandFlipd( keys=[\"image_dce\", ], spatial_axis=[1], prob=0.50),\n",
    "            RandFlipd( keys=[\"image_dce\", ], spatial_axis=[2], prob=0.50),\n",
    "            \n",
    "            RandRotate90d(keys=[\"image_adc\", 'image_dce'], prob=0.50, max_k=3 ),\n",
    "            RandShiftIntensityd( keys=[\"image_adc\", 'image_dce'], offsets=0.10, prob=0.50),\n",
    "            \n",
    "            ToTensord(keys=['image_adc', 'image_dce','clinical',  'label'])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image_adc\",'image_dce' ]),\n",
    "            EnsureChannelFirstd(keys=[\"image_adc\", 'image_dce']),\n",
    "            Orientationd(keys=[\"image_adc\",'image_dce'], axcodes=\"RAS\"),\n",
    "            Resized(keys=[\"image_adc\"], spatial_size=(64, 64, 16)),\n",
    "            Resized(keys=[\"image_dce\"], spatial_size=(96, 96, 32)),\n",
    "            \n",
    "            NormalizeIntensityd(keys=[\"image_adc\", 'image_dce'], nonzero=True, channel_wise=True),\n",
    "            ToTensord(keys=['image_adc', 'image_dce','clinical', 'label'])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "train_ds = CacheDataset(data=train_dict, transform=val_transforms, cache_rate=1.0, num_workers=24)\n",
    "val_ds = CacheDataset(data=val_dict, transform=val_transforms, cache_rate=1.0, num_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training data loader\n",
    "train_loader = DataLoader(train_ds, batch_size=12, num_workers=16, pin_memory=True)\n",
    "\n",
    "# create a validation data loader\n",
    "val_loader = DataLoader(val_ds, batch_size=12, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DoubleTower(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_dce='', \n",
    "                 pretrained_adc='', \n",
    "                 device = torch.device(\"cuda\"),\n",
    "                 num_classes=2, \n",
    "                 fc_hidden_size = 128\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.pretrained_dce = pretrained_dce\n",
    "        self.pretrained_adc = pretrained_adc\n",
    "        self.fc_hidden_size = fc_hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        self.model_dce = monai.networks.nets.resnet34(spatial_dims=3, n_input_channels=1, num_classes=2, feed_forward=False).to(self.device)\n",
    "        self.model_adc = monai.networks.nets.resnet34(spatial_dims=3, n_input_channels=1, num_classes=2, feed_forward=False).to(self.device)\n",
    "        \n",
    "        if  pretrained_dce != '':\n",
    "            dce_dict = self.model_dce.state_dict()\n",
    "            dce_pretrain = torch.load(self.pretrained_dce, map_location=self.device)\n",
    "            dce_pretrain_dict = {k:v for k, v in dce_pretrain.items() if  k in  dce_dict.keys()}\n",
    "            dce_dict.update(dce_pretrain_dict)\n",
    "            self.model_dce.load_state_dict(dce_dict)\n",
    "\n",
    "        if  pretrained_adc !='':\n",
    "            adc_dict = self.model_adc.state_dict()\n",
    "            adc_pretrain = torch.load(self.pretrained_adc, map_location=self.device)\n",
    "            adc_pretrain_dict = {k:v for k, v in adc_pretrain.items() if  k in  adc_dict.keys()}\n",
    "            adc_dict.update(adc_pretrain_dict)\n",
    "            self.model_adc.load_state_dict(adc_dict)\n",
    "        \n",
    "        \n",
    "        # self.Linear1 = Linear(1024 + 6, self.num_classes, device=self.device) \n",
    "        self.Linear1 = Linear(1024, self.fc_hidden_size, device=self.device)  # 1024 是 所有下采样特征图globalpool之后拼接的结果        \n",
    "        self.Linear2 = Linear(self.fc_hidden_size + 6, self.num_classes, device=self.device)  \n",
    "        \n",
    "    \n",
    "    def forward(self, x1, x2, structured_data):  # x 是SegResNet的输入影像矩阵\n",
    "        \n",
    "        encode_output1 = self.model_dce(x1)\n",
    "        encode_output2 = self.model_dce(x2)\n",
    "        \n",
    "        concatenated = torch.concat([encode_output1, encode_output2], dim=-1)\n",
    "        \n",
    "        fc1 = F.relu(self.Linear1(concatenated)) \n",
    "        # fc1 = nn.Dropout(0.2)(fc1)\n",
    "       \n",
    "        fc2 = self.Linear2( torch.concat([fc1, structured_data], dim=-1))\n",
    "        return F.log_softmax(fc2, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dce_pretrain_path = ''\n",
    "adc_pretrain_path = ''\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DoubleTower(dce_pretrain_path, adc_pretrain_path, device = device)\n",
    "\n",
    "pretrained_path = './Dropout/best_metric_model_classification3d_dict.pth'\n",
    "model.load_state_dict(torch.load(pretrained_path, map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average  train loss: 0.5412\n",
      "saved new best metric model\n",
      "current epoch: 1 current accuracy: 0.7020 current AUC: 0.7969 best accuracy: 0.7969 at epoch 1\n",
      "tensor([[-0.6239, -0.7676],\n",
      "        [-0.1130, -2.2361]], device='cuda:1')\n",
      "epoch 2 average  train loss: 0.4585\n",
      "saved new best metric model\n",
      "current epoch: 2 current accuracy: 0.6887 current AUC: 0.8037 best accuracy: 0.8037 at epoch 2\n",
      "tensor([[-7.7733e-02, -2.5931e+00],\n",
      "        [-1.5589e-03, -6.4646e+00]], device='cuda:1')\n",
      "epoch 3 average  train loss: 0.3625\n",
      "current epoch: 3 current accuracy: 0.6887 current AUC: 0.7943 best accuracy: 0.8037 at epoch 2\n",
      "tensor([[-8.9241e-03, -4.7235e+00],\n",
      "        [-1.4490e-03, -6.5376e+00]], device='cuda:1')\n",
      "epoch 4 average  train loss: 0.3429\n",
      "current epoch: 4 current accuracy: 0.6755 current AUC: 0.7691 best accuracy: 0.8037 at epoch 2\n",
      "tensor([[-1.4233e-04, -8.8576e+00],\n",
      "        [ 0.0000e+00, -1.8217e+01]], device='cuda:1')\n",
      "epoch 5 average  train loss: 0.2827\n",
      "current epoch: 5 current accuracy: 0.6854 current AUC: 0.7666 best accuracy: 0.8037 at epoch 2\n",
      "tensor([[-3.1943e-04, -8.0491e+00],\n",
      "        [-4.7529e-04, -7.6517e+00]], device='cuda:1')\n",
      "train completed, best_metric: 0.8037 at epoch: 2\n"
     ]
    }
   ],
   "source": [
    "post_pred = Compose([Activations(softmax=True)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "auc_metric = ROCAUCMetric()\n",
    "\n",
    "# start a typical PyTorch training\n",
    "val_interval = 1\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "max_epochs = 150\n",
    "for epoch in range(max_epochs):\n",
    " \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    val_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        input_dce, input_adc, input_clinical, labels = batch_data[\"image_dce\"].to(device), batch_data['image_adc'].to(device), batch_data[\"clinical\"].to(device), batch_data[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_dce, input_adc, input_clinical)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "\n",
    "    epoch_loss /= step\n",
    "    print(f\"epoch {epoch + 1} average  train loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "       \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            \n",
    "            step2 = 0\n",
    "            for val_data in val_loader:\n",
    "                step2 += 1\n",
    "                val_dce, val_adc, val_clinical, val_labels = val_data[\"image_dce\"].to(device),val_data[\"image_adc\"].to(device), val_data[\"clinical\"].to(device), val_data[\"label\"].to(device)\n",
    "                val_output = model(val_dce, val_adc, val_clinical)\n",
    "                y_pred = torch.cat([y_pred, val_output], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "                val_loss += loss_function(val_output, val_labels).item()\n",
    "                \n",
    "            val_loss /= step2\n",
    "            \n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            y_onehot = [post_label(i) for i in decollate_batch(y, detach=False)]\n",
    "            y_pred_act = [post_pred(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            auc_result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            if auc_result > best_metric:\n",
    "                best_metric = auc_result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"best_metric_model_classification3d_dict.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                \"current epoch: {} current accuracy: {:.4f} current AUC: {:.4f} best accuracy: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, acc_metric, auc_result, best_metric, best_metric_epoch\n",
    "                )\n",
    "            )\n",
    "            print(val_output)\n",
    "    if epoch > 3:\n",
    "        break\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0018,  0.0067,  0.0166], device='cuda:1')\n",
      "tensor([[-9.5228e-03, -4.6588e+00],\n",
      "        [-1.7582e-04, -8.6462e+00],\n",
      "        [-2.5987e-05, -1.0559e+01],\n",
      "        [-2.4063e-03, -6.0308e+00],\n",
      "        [-6.3670e-01, -7.5297e-01],\n",
      "        [-2.4522e-02, -3.7204e+00],\n",
      "        [-4.8411e-01, -9.5775e-01],\n",
      "        [-6.4882e-03, -5.0410e+00],\n",
      "        [-1.2420e-03, -6.6917e+00],\n",
      "        [-5.4836e-06, -1.2119e+01],\n",
      "        [-1.2413e-02, -4.3952e+00],\n",
      "        [-1.7677e-04, -8.6408e+00]], device='cuda:1')\n",
      "tensor([[-3.1168e-04, -8.0736e+00],\n",
      "        [-2.9440e-01, -1.3664e+00],\n",
      "        [-1.2053e-02, -4.4244e+00],\n",
      "        [-1.0921e-03, -6.8203e+00],\n",
      "        [-4.7563e-05, -9.9541e+00],\n",
      "        [-1.0266e-02, -4.5840e+00],\n",
      "        [-1.7068e-01, -1.8521e+00],\n",
      "        [-8.4742e-03, -4.7750e+00],\n",
      "        [-1.6891e-04, -8.6861e+00],\n",
      "        [-3.8628e-04, -7.8591e+00],\n",
      "        [-4.9232e-05, -9.9199e+00],\n",
      "        [-1.6700e-04, -8.6976e+00]], device='cuda:1')\n",
      "tensor([[-7.1954e-03, -4.9379e+00],\n",
      "        [-3.4987e-03, -5.6571e+00],\n",
      "        [-1.8835e-05, -1.0882e+01],\n",
      "        [-9.7751e-06, -1.1536e+01],\n",
      "        [-1.1193e-04, -9.0975e+00],\n",
      "        [-6.4649e-03, -5.0446e+00],\n",
      "        [-5.1855e-04, -7.5648e+00],\n",
      "        [-2.6568e-02, -3.6413e+00],\n",
      "        [-2.1225e-02, -3.8631e+00],\n",
      "        [-4.3698e-01, -1.0384e+00],\n",
      "        [-8.7022e-06, -1.1654e+01],\n",
      "        [-3.2521e-02, -3.4421e+00]], device='cuda:1')\n",
      "tensor([[-6.5859e-02, -2.7530e+00],\n",
      "        [-4.2892e-01, -1.0533e+00],\n",
      "        [-8.3411e-02, -2.5254e+00],\n",
      "        [-4.0733e-01, -1.0949e+00],\n",
      "        [-1.5855e-05, -1.1053e+01],\n",
      "        [-2.4438e-05, -1.0619e+01],\n",
      "        [-6.4269e-04, -7.3502e+00],\n",
      "        [-3.5763e-06, -1.2535e+01],\n",
      "        [-1.1367e-02, -4.4828e+00],\n",
      "        [-4.8762e-01, -9.5215e-01],\n",
      "        [-2.1026e+00, -1.3027e-01],\n",
      "        [-2.3530e-02, -3.7612e+00]], device='cuda:1')\n",
      "tensor([[-2.0222e-03, -6.2046e+00],\n",
      "        [-6.3326e-01, -7.5685e-01],\n",
      "        [-1.3220e+00, -3.1005e-01],\n",
      "        [-4.9590e-05, -9.9113e+00],\n",
      "        [-7.7635e-04, -7.1612e+00],\n",
      "        [-5.1294e-02, -2.9957e+00],\n",
      "        [ 0.0000e+00, -1.8583e+01],\n",
      "        [-2.4935e-04, -8.2968e+00],\n",
      "        [-2.2936e-02, -3.7865e+00],\n",
      "        [-2.8109e-03, -5.8756e+00],\n",
      "        [-8.9792e-02, -2.4548e+00],\n",
      "        [-3.3560e-02, -3.4112e+00]], device='cuda:1')\n",
      "tensor([[-5.2570e-05, -9.8540e+00],\n",
      "        [-4.1166e-04, -7.7956e+00],\n",
      "        [-8.8938e-04, -7.0254e+00],\n",
      "        [ 0.0000e+00, -1.7988e+01],\n",
      "        [-4.0848e-01, -1.0926e+00],\n",
      "        [-1.0115e-02, -4.5988e+00],\n",
      "        [-2.3842e-07, -1.5184e+01],\n",
      "        [-3.2052e-03, -5.7446e+00],\n",
      "        [-1.0860e-02, -4.5281e+00],\n",
      "        [-5.9605e-07, -1.4419e+01],\n",
      "        [-6.6585e-03, -5.0152e+00],\n",
      "        [-5.7776e-01, -8.2360e-01]], device='cuda:1')\n",
      "tensor([[ 0.0000e+00, -1.6854e+01],\n",
      "        [-3.1967e-02, -3.4590e+00],\n",
      "        [-1.2785e-02, -4.3659e+00],\n",
      "        [-3.4434e-04, -7.9740e+00],\n",
      "        [-2.1338e-05, -1.0754e+01],\n",
      "        [-2.2416e-03, -6.1017e+00],\n",
      "        [-3.3252e-03, -5.7079e+00],\n",
      "        [-5.7211e-03, -5.1664e+00],\n",
      "        [-4.8994e-05, -9.9245e+00],\n",
      "        [-7.1684e-02, -2.6711e+00],\n",
      "        [-1.2145e-01, -2.1684e+00],\n",
      "        [-1.1325e-05, -1.1386e+01]], device='cuda:1')\n",
      "tensor([[-2.6758e-03, -5.9248e+00],\n",
      "        [-2.3495e-03, -6.0547e+00],\n",
      "        [-7.0270e-02, -2.6903e+00],\n",
      "        [-3.3497e-05, -1.0305e+01],\n",
      "        [-1.1921e-06, -1.3623e+01],\n",
      "        [-9.3337e-05, -9.2798e+00],\n",
      "        [-1.6778e-03, -6.3911e+00],\n",
      "        [-8.8510e-03, -4.7317e+00],\n",
      "        [-1.6380e-02, -4.1199e+00],\n",
      "        [-2.1241e-04, -8.4571e+00],\n",
      "        [-7.7486e-06, -1.1775e+01],\n",
      "        [-1.7928e-01, -1.8071e+00]], device='cuda:1')\n",
      "tensor([[-8.8214e-06, -1.1634e+01],\n",
      "        [-8.8423e-01, -5.3278e-01],\n",
      "        [-1.3783e-01, -2.0499e+00],\n",
      "        [-9.5367e-07, -1.3852e+01],\n",
      "        [-1.7158e-02, -4.0739e+00],\n",
      "        [-4.0625e-02, -3.2236e+00],\n",
      "        [-3.3519e-03, -5.6999e+00],\n",
      "        [-6.8069e-02, -2.7211e+00],\n",
      "        [-1.8244e-01, -1.7912e+00],\n",
      "        [-6.6757e-06, -1.1912e+01],\n",
      "        [-8.5860e-03, -4.7619e+00],\n",
      "        [-1.8497e-01, -1.7786e+00]], device='cuda:1')\n",
      "tensor([[-4.2080e-05, -1.0076e+01],\n",
      "        [-9.4089e-03, -4.6708e+00],\n",
      "        [-1.4406e+00, -2.7020e-01],\n",
      "        [-8.9599e-03, -4.7195e+00],\n",
      "        [-5.7989e-01, -8.2089e-01],\n",
      "        [-9.4214e-04, -6.9678e+00],\n",
      "        [-1.4367e-01, -2.0112e+00],\n",
      "        [-1.4474e-02, -4.2426e+00],\n",
      "        [-2.7742e-02, -3.5987e+00],\n",
      "        [-1.5017e-01, -1.9701e+00],\n",
      "        [-4.3391e-05, -1.0045e+01],\n",
      "        [-1.9073e-06, -1.3198e+01]], device='cuda:1')\n",
      "tensor([[-9.2362e-03, -4.6892e+00],\n",
      "        [-4.0749e-04, -7.8057e+00],\n",
      "        [-1.4876e-04, -8.8136e+00],\n",
      "        [-1.7363e-02, -4.0621e+00],\n",
      "        [-1.2246e-03, -6.7057e+00],\n",
      "        [-1.7178e-02, -4.0727e+00],\n",
      "        [-2.1470e+00, -1.2424e-01],\n",
      "        [-2.1272e-01, -1.6523e+00],\n",
      "        [-6.5376e-03, -5.0334e+00],\n",
      "        [-2.6166e-02, -3.6563e+00],\n",
      "        [-6.8470e-03, -4.9874e+00],\n",
      "        [-2.1212e-01, -1.6548e+00]], device='cuda:1')\n",
      "tensor([[-1.9026e-01, -1.7530e+00],\n",
      "        [-1.4802e-02, -4.2204e+00],\n",
      "        [-1.8594e-02, -3.9942e+00],\n",
      "        [-1.5562e+00, -2.3690e-01],\n",
      "        [-3.2196e-01, -1.2900e+00],\n",
      "        [-4.0703e-02, -3.2217e+00],\n",
      "        [-3.4987e-02, -3.3702e+00],\n",
      "        [-1.1744e-01, -2.2000e+00],\n",
      "        [-2.0883e-01, -1.6688e+00],\n",
      "        [-2.2722e-02, -3.7958e+00],\n",
      "        [-6.4373e-06, -1.1960e+01],\n",
      "        [-2.4529e-03, -6.0117e+00]], device='cuda:1')\n",
      "tensor([[-1.1358e-03, -6.7810e+00],\n",
      "        [-4.9992e-03, -5.3010e+00],\n",
      "        [-1.9921e+00, -1.4666e-01],\n",
      "        [-4.5778e-03, -5.3888e+00],\n",
      "        [-6.5215e-02, -2.7625e+00],\n",
      "        [-8.3446e-07, -1.3979e+01],\n",
      "        [-4.5434e-02, -3.1141e+00],\n",
      "        [-1.1379e-03, -6.7792e+00],\n",
      "        [-6.8435e-01, -7.0203e-01],\n",
      "        [-5.8649e-05, -9.7446e+00],\n",
      "        [-3.4858e-02, -3.3738e+00],\n",
      "        [-1.1921e-07, -1.6560e+01]], device='cuda:1')\n",
      "tensor([[-4.6725e-02, -3.0867e+00],\n",
      "        [-6.4483e-03, -5.0472e+00],\n",
      "        [-1.0442e+00, -4.3380e-01],\n",
      "        [-1.4352e-04, -8.8492e+00],\n",
      "        [-3.9271e-02, -3.2568e+00],\n",
      "        [-1.3178e-02, -4.3358e+00],\n",
      "        [-7.9509e-05, -9.4401e+00],\n",
      "        [ 0.0000e+00, -2.5517e+01],\n",
      "        [-5.0027e-01, -9.3233e-01],\n",
      "        [-9.8826e-03, -4.6219e+00],\n",
      "        [-1.0854e-01, -2.2744e+00],\n",
      "        [-2.2567e-03, -6.0950e+00]], device='cuda:1')\n",
      "tensor([[-4.0860e-01, -1.0924e+00],\n",
      "        [-2.9802e-06, -1.2729e+01],\n",
      "        [-5.3908e-01, -8.7535e-01],\n",
      "        [-1.6677e-01, -1.8734e+00],\n",
      "        [-1.2798e+00, -3.2584e-01],\n",
      "        [-2.5852e-02, -3.6683e+00],\n",
      "        [-1.9290e-02, -3.9578e+00],\n",
      "        [-1.9860e+00, -1.4763e-01],\n",
      "        [-9.1791e-06, -1.1594e+01],\n",
      "        [-8.8569e-05, -9.3318e+00],\n",
      "        [-1.4471e-04, -8.8412e+00],\n",
      "        [-3.7527e-01, -1.1619e+00]], device='cuda:1')\n",
      "tensor([[-1.7780e-02, -4.0386e+00],\n",
      "        [-7.0523e-04, -7.2573e+00],\n",
      "        [-2.5877e-04, -8.2595e+00],\n",
      "        [-5.1339e-03, -5.2745e+00],\n",
      "        [-1.7545e-01, -1.8268e+00],\n",
      "        [-5.5557e-02, -2.9180e+00],\n",
      "        [-7.4116e-01, -6.4733e-01],\n",
      "        [-1.6589e-01, -1.8782e+00],\n",
      "        [-2.4107e-03, -6.0290e+00],\n",
      "        [-1.8358e-05, -1.0904e+01],\n",
      "        [-1.0156e-04, -9.1948e+00],\n",
      "        [-1.1375e+00, -3.8660e-01]], device='cuda:1')\n",
      "tensor([[-3.1276e-03, -5.7691e+00],\n",
      "        [-8.4172e-01, -5.6382e-01],\n",
      "        [-7.5297e-03, -4.8927e+00],\n",
      "        [-6.5430e-03, -5.0326e+00],\n",
      "        [-5.4467e-03, -5.2155e+00],\n",
      "        [-1.8597e-01, -1.7737e+00],\n",
      "        [-1.6342e-04, -8.7193e+00],\n",
      "        [-1.6322e-03, -6.4186e+00],\n",
      "        [-8.1848e-02, -2.5435e+00],\n",
      "        [-7.0583e-01, -6.8062e-01],\n",
      "        [-9.4989e-04, -6.9597e+00],\n",
      "        [-8.2013e-05, -9.4081e+00]], device='cuda:1')\n",
      "tensor([[-4.3034e-05, -1.0053e+01],\n",
      "        [-1.4742e-01, -1.9873e+00],\n",
      "        [-3.0994e-05, -1.0383e+01],\n",
      "        [-1.1527e-04, -9.0684e+00],\n",
      "        [-9.2991e-03, -4.6825e+00],\n",
      "        [-8.8582e-03, -4.7308e+00],\n",
      "        [-1.1921e-06, -1.3634e+01],\n",
      "        [-7.4599e-02, -2.6327e+00],\n",
      "        [-7.8732e-03, -4.8482e+00],\n",
      "        [-2.5005e-01, -1.5085e+00],\n",
      "        [-1.0133e-05, -1.1504e+01],\n",
      "        [-1.0521e+00, -4.2955e-01]], device='cuda:1')\n",
      "tensor([[-7.1606e-02, -2.6722e+00],\n",
      "        [-3.7582e-03, -5.5857e+00],\n",
      "        [-1.1921e-07, -1.6471e+01],\n",
      "        [-9.6421e-03, -4.6464e+00],\n",
      "        [-2.0854e-02, -3.8806e+00],\n",
      "        [-4.3810e-01, -1.0364e+00],\n",
      "        [-1.9935e-02, -3.9252e+00],\n",
      "        [-1.6166e-03, -6.4282e+00],\n",
      "        [-2.0681e-04, -8.4841e+00],\n",
      "        [-3.0744e-01, -1.3293e+00],\n",
      "        [-8.1177e-02, -2.5514e+00],\n",
      "        [-4.3974e-02, -3.1461e+00]], device='cuda:1')\n",
      "tensor([[-6.8748e-04, -7.2828e+00],\n",
      "        [-3.3182e-03, -5.7100e+00],\n",
      "        [-5.6875e-03, -5.1723e+00],\n",
      "        [-6.0885e-04, -7.4042e+00],\n",
      "        [-1.4408e-02, -4.2472e+00],\n",
      "        [-1.3253e-01, -2.0865e+00],\n",
      "        [-5.1742e-02, -2.9872e+00],\n",
      "        [-1.2994e-05, -1.1250e+01],\n",
      "        [-1.0756e+00, -4.1715e-01],\n",
      "        [-1.7852e-02, -4.0346e+00],\n",
      "        [-3.5047e-03, -5.6554e+00],\n",
      "        [-1.1943e-02, -4.4336e+00]], device='cuda:1')\n",
      "tensor([[-8.7348e-01, -5.4042e-01],\n",
      "        [-4.1068e-01, -1.0883e+00],\n",
      "        [-1.6307e-04, -8.7213e+00],\n",
      "        [-2.5191e-02, -3.6938e+00],\n",
      "        [-3.0994e-06, -1.2673e+01],\n",
      "        [-1.9907e-02, -3.9266e+00],\n",
      "        [-7.0367e-02, -2.6890e+00],\n",
      "        [-5.9605e-07, -1.4373e+01],\n",
      "        [-3.8185e-03, -5.5698e+00],\n",
      "        [-6.1070e-02, -2.8261e+00],\n",
      "        [-1.1772e-02, -4.4479e+00],\n",
      "        [-1.2829e-02, -4.3624e+00]], device='cuda:1')\n",
      "tensor([[-1.1535e-03, -6.7656e+00],\n",
      "        [-2.9363e-03, -5.8320e+00],\n",
      "        [-3.2186e-06, -1.2660e+01],\n",
      "        [-9.4088e-03, -4.6708e+00],\n",
      "        [-5.5312e-05, -9.8025e+00],\n",
      "        [-1.1741e-04, -9.0502e+00],\n",
      "        [-2.6353e-01, -1.4625e+00],\n",
      "        [-2.7891e-04, -8.1850e+00],\n",
      "        [-1.6337e-01, -1.8923e+00],\n",
      "        [-3.3379e-06, -1.2603e+01],\n",
      "        [-3.7306e-04, -7.8941e+00],\n",
      "        [-1.0252e-05, -1.1488e+01]], device='cuda:1')\n",
      "tensor([[-1.2453e-01, -2.1448e+00],\n",
      "        [-2.8406e-02, -3.5753e+00],\n",
      "        [-1.2983e-03, -6.6474e+00],\n",
      "        [-1.6509e-04, -8.7089e+00],\n",
      "        [-1.2662e-03, -6.6724e+00],\n",
      "        [ 0.0000e+00, -1.7279e+01],\n",
      "        [-6.0846e-03, -5.1050e+00],\n",
      "        [-3.0898e-01, -1.3250e+00],\n",
      "        [-2.8313e-01, -1.4001e+00],\n",
      "        [-4.1393e-03, -5.4893e+00],\n",
      "        [-1.6002e-01, -1.9114e+00],\n",
      "        [-1.1657e-01, -2.2070e+00]], device='cuda:1')\n",
      "tensor([[-6.9546e-01, -6.9084e-01],\n",
      "        [ 0.0000e+00, -1.6977e+01],\n",
      "        [-1.7020e+00, -2.0128e-01],\n",
      "        [-1.1494e-02, -4.4717e+00],\n",
      "        [-1.2711e-01, -2.1255e+00],\n",
      "        [-4.4631e-03, -5.4141e+00],\n",
      "        [-5.4756e-03, -5.2102e+00],\n",
      "        [-3.0246e-03, -5.8025e+00],\n",
      "        [-3.5635e-03, -5.6388e+00],\n",
      "        [-3.9681e-01, -1.1162e+00],\n",
      "        [-2.3495e-01, -1.5636e+00],\n",
      "        [-3.3284e-02, -3.4193e+00]], device='cuda:1')\n",
      "tensor([[-1.3019e-01, -2.1032e+00],\n",
      "        [-1.1409e-01, -2.2273e+00],\n",
      "        [-1.0081e-02, -4.6021e+00],\n",
      "        [-3.2667e-03, -5.7256e+00],\n",
      "        [-2.9199e-02, -3.5482e+00],\n",
      "        [-7.9870e-06, -1.1744e+01],\n",
      "        [-3.5246e-01, -1.2139e+00],\n",
      "        [-5.0199e-03, -5.2969e+00],\n",
      "        [-2.8964e-01, -1.3804e+00],\n",
      "        [-5.3409e-01, -8.8239e-01],\n",
      "        [-9.8231e-02, -2.3691e+00],\n",
      "        [-1.0313e+00, -4.4090e-01]], device='cuda:1')\n",
      "tensor([[-7.7733e-02, -2.5931e+00],\n",
      "        [-1.5589e-03, -6.4646e+00]], device='cuda:1')\n",
      "current epoch: 5 current accuracy: 0.6887 current AUC: 0.8037 best accuracy: 0.8037 at epoch 2\n",
      "tensor([[-7.7733e-02, -2.5931e+00],\n",
      "        [-1.5589e-03, -6.4646e+00]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DoubleTower(dce_pretrain_path, adc_pretrain_path, device = device)\n",
    "\n",
    "pretrained_path = './best_metric_model_classification3d_dict.pth'\n",
    "model.load_state_dict(torch.load(pretrained_path, map_location=device))\n",
    "print(model.state_dict()['model_dce.conv1.weight'][0, 0, 0, 0, :3])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "    y = torch.tensor([], dtype=torch.long, device=device)\n",
    "\n",
    "    step2 = 0\n",
    "    for val_data in val_loader:\n",
    "        step2 += 1\n",
    "        val_dce, val_adc, val_clinical, val_labels = val_data[\"image_dce\"].to(device),val_data[\"image_adc\"].to(device), val_data[\"clinical\"].to(device), val_data[\"label\"].to(device)\n",
    "        \n",
    "        val_output = model(val_dce, val_adc, val_clinical)\n",
    "        print(val_output)\n",
    "        y_pred = torch.cat([y_pred, val_output], dim=0)\n",
    "        y = torch.cat([y, val_labels], dim=0)\n",
    "        val_loss += loss_function(val_output, val_labels).item()\n",
    "\n",
    "    val_loss /= step2\n",
    "\n",
    "    acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "    acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "    y_onehot = [post_label(i) for i in decollate_batch(y, detach=False)]\n",
    "    y_pred_act = [post_pred(i) for i in decollate_batch(y_pred)]\n",
    "    auc_metric(y_pred_act, y_onehot)\n",
    "    auc_result = auc_metric.aggregate()\n",
    "    auc_metric.reset()\n",
    "    del y_pred_act, y_onehot\n",
    "    \n",
    "    print(\n",
    "        \"current epoch: {} current accuracy: {:.4f} current AUC: {:.4f} best accuracy: {:.4f} at epoch {}\".format(\n",
    "            epoch + 1, acc_metric, auc_result, best_metric, best_metric_epoch\n",
    "        )\n",
    "    )\n",
    "    print(val_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2887209575.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[55], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    current epoch: 2 current accuracy: 0.7318 current AUC: 0.8203 best accuracy: 0.8213 at epoch 1\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "current epoch: 2 current accuracy: 0.7318 current AUC: 0.8203 best accuracy: 0.8213 at epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dce_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0018, device='cuda:1')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
